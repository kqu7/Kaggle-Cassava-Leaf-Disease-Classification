{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "kqu_cassava_leaf_inference.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ky3zJJbsvXl_"
      },
      "source": [
        "# Configuration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dgzB8YWyvDR9"
      },
      "source": [
        "class CFG:\n",
        "    # -----------\n",
        "    # Environment\n",
        "    # ----------- \n",
        "    platform = 'colab'  # Options: ['kaggle', 'colab']\n",
        "    device = 'cuda'  # Options: ['cuda', 'cpu']\n",
        "\n",
        "    # -----\n",
        "    # Paths\n",
        "    # -----\n",
        "    test_data_path = '/content/gdrive/MyDrive/kaggle/kaggle-competition-datasets/cassava-leaf-disease-classification/'\n",
        "    test_img_path = '/content/test/'\n",
        "    model_path = '/content/gdrive/MyDrive/kaggle/kaggle-models/kaggle-leaf-classification-models/pretrained_models/'\n",
        "    log_path = '/content/gdrive/MyDrive/kaggle/kaggle-models/kaggle-leaf-classification-models/'\n",
        "\n",
        "    # ----\n",
        "    # Data\n",
        "    # ----\n",
        "    n_classes = 5  # Indicates the number of classes for this classification task\n",
        "    img_size = 384  # Options: [384x384, 512x512]; if VIT or deit is chosen as model, need 384 x 384\n",
        "    n_epochs = 5  # Indicates the number of epochs trained\n",
        "    n_folds = 5  # Indicates the number of k-cross validation\n",
        "    num_workers = 2\n",
        "    batch_size = 1\n",
        "\n",
        "    # -----------------\n",
        "    # Pretrained Models\n",
        "    # -----------------\n",
        "    model_list = [0, 1, 2, 3] # Indicates the indices of pretrained models used for inference\n",
        "    use_TTA = True  \n",
        "    n_TTA = 8\n",
        "    pretrained = False\n",
        "    debug = True\n",
        "\n",
        "\n",
        "    seed = 42\n",
        "\n",
        "if CFG.use_TTA==True:\n",
        "  assert CFG.batch_size == 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eSAdJqgtvRw0"
      },
      "source": [
        "# Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9w4ecPgbt1vG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d6c6b509-4f8f-4b23-8ce3-d5cbc0a74953"
      },
      "source": [
        "if CFG.platform == 'colab':\n",
        "  !pip install tqdm --upgrade\n",
        "  !pip install -U albumentations\n",
        "  !pip install timm\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "from datetime import datetime\n",
        "from zipfile import ZipFile\n",
        "import random\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "from logging import Formatter, StreamHandler, getLogger\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "import torch \n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.utils.data.sampler import WeightedRandomSampler\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "\n",
        "import timm\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm.notebook import tqdm\n",
        "from google.colab import drive\n",
        "    \n",
        "if CFG.platform == 'colab':\n",
        "  package_paths = ['/content/gdrive/MyDrive//kaggle/kaggle-models/efficientnet_pytorch-0.7.0',\n",
        "                  '/content/gdrive/MyDrive/kaggle/kaggle-models/FMix-master']\n",
        "else:\n",
        "  package_paths = ['/kaggle/input/timm-pytorch-image-models/pytorch-image-models-master']\n",
        "\n",
        "for path in package_paths: \n",
        "  sys.path.append(path)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (4.62.0)\n",
            "Collecting tqdm\n",
            "  Downloading tqdm-4.62.2-py2.py3-none-any.whl (76 kB)\n",
            "\u001b[?25l\r\u001b[K     |████▎                           | 10 kB 41.9 MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 20 kB 2.1 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 30 kB 3.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 40 kB 3.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▌          | 51 kB 3.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 61 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 71 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 76 kB 2.7 MB/s \n",
            "\u001b[?25hInstalling collected packages: tqdm\n",
            "  Attempting uninstall: tqdm\n",
            "    Found existing installation: tqdm 4.62.0\n",
            "    Uninstalling tqdm-4.62.0:\n",
            "      Successfully uninstalled tqdm-4.62.0\n",
            "Successfully installed tqdm-4.62.2\n",
            "Requirement already satisfied: albumentations in /usr/local/lib/python3.7/dist-packages (0.1.12)\n",
            "Collecting albumentations\n",
            "  Downloading albumentations-1.0.3-py3-none-any.whl (98 kB)\n",
            "\u001b[K     |████████████████████████████████| 98 kB 4.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from albumentations) (3.13)\n",
            "Collecting opencv-python-headless>=4.1.1\n",
            "  Downloading opencv_python_headless-4.5.3.56-cp37-cp37m-manylinux2014_x86_64.whl (37.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 37.1 MB 1.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from albumentations) (1.19.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from albumentations) (1.4.1)\n",
            "Requirement already satisfied: scikit-image>=0.16.1 in /usr/local/lib/python3.7/dist-packages (from albumentations) (0.16.2)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.16.1->albumentations) (2.6.2)\n",
            "Requirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.16.1->albumentations) (2.4.1)\n",
            "Requirement already satisfied: matplotlib!=3.0.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.16.1->albumentations) (3.2.2)\n",
            "Requirement already satisfied: PyWavelets>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.16.1->albumentations) (1.1.1)\n",
            "Requirement already satisfied: pillow>=4.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.16.1->albumentations) (7.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations) (2.8.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations) (0.10.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations) (2.4.7)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from cycler>=0.10->matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations) (1.15.0)\n",
            "Installing collected packages: opencv-python-headless, albumentations\n",
            "  Attempting uninstall: albumentations\n",
            "    Found existing installation: albumentations 0.1.12\n",
            "    Uninstalling albumentations-0.1.12:\n",
            "      Successfully uninstalled albumentations-0.1.12\n",
            "Successfully installed albumentations-1.0.3 opencv-python-headless-4.5.3.56\n",
            "Collecting timm\n",
            "  Downloading timm-0.4.12-py3-none-any.whl (376 kB)\n",
            "\u001b[K     |████████████████████████████████| 376 kB 8.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from timm) (0.10.0+cu102)\n",
            "Requirement already satisfied: torch>=1.4 in /usr/local/lib/python3.7/dist-packages (from timm) (1.9.0+cu102)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.4->timm) (3.7.4.3)\n",
            "Requirement already satisfied: pillow>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->timm) (7.1.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision->timm) (1.19.5)\n",
            "Installing collected packages: timm\n",
            "Successfully installed timm-0.4.12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LhI8MCGUWG9Y"
      },
      "source": [
        "# Data Import"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o_jA3eWsWGmq",
        "outputId": "b4a46c2c-fa2c-468d-c6f1-a94853326e19"
      },
      "source": [
        "if CFG.platform == 'colab':\n",
        "  drive.mount('/content/gdrive')\n",
        "\n",
        "  test_zip_path = CFG.test_data_path + \"test.zip\"\n",
        "  test_img_path = CFG.test_img_path\n",
        "\n",
        "  if not os.path.isdir(test_img_path):\n",
        "    with ZipFile(test_zip_path, 'r') as zip_f: \n",
        "      zip_f.extractall(path='/content') \n",
        "else:\n",
        "  test_img_path = '../input/cassava-leaf-disease-classification/test_images/'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tBhrBYRulvwe"
      },
      "source": [
        "# Utils"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2JGXR7j6lxhB"
      },
      "source": [
        "def seed_everything(seed=CFG.seed):\n",
        "  random.seed(seed)\n",
        "  os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "  np.random.seed(seed)\n",
        "  \n",
        "  torch.manual_seed(seed)\n",
        "  torch.cuda.manual_seed(seed)\n",
        "  torch.backends.cudnn.deterministic = True\n",
        "  torch.backends.cudnn.benchmark = True\n",
        "\n",
        "def read_img_from_path(path):\n",
        "  im_bgr = cv2.imread(path)\n",
        "  im_rgb = im_bgr[:, :, ::-1].copy()\n",
        "  return im_rgb"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dx_ejkUbvZDC"
      },
      "source": [
        "# Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5zRhGIa3v3r3"
      },
      "source": [
        "from albumentations.pytorch import ToTensorV2\n",
        "from albumentations import (\n",
        "    HorizontalFlip, VerticalFlip, IAAPerspective, ShiftScaleRotate, CLAHE, RandomRotate90,\n",
        "    Transpose, ShiftScaleRotate, Blur, OpticalDistortion, GridDistortion, HueSaturationValue,\n",
        "    IAAAdditiveGaussianNoise, GaussNoise, MotionBlur, MedianBlur, IAAPiecewiseAffine, RandomResizedCrop,\n",
        "    IAASharpen, IAAEmboss, RandomBrightnessContrast, Flip, OneOf, Compose, Normalize, Cutout, CoarseDropout, ShiftScaleRotate, CenterCrop, Resize\n",
        ")\n",
        "\n",
        "def get_heavy_transforms():\n",
        "  train_transforms = Compose(\n",
        "      [\n",
        "        RandomResizedCrop(CFG.img_size, CFG.img_size),\n",
        "        HorizontalFlip(p=0.5),\n",
        "        VerticalFlip(p=0.5),\n",
        "        ShiftScaleRotate(p=0.1),\n",
        "        RandomBrightnessContrast(brightness_limit=(-0.1,0.1), contrast_limit=(-0.1, 0.1), p=0.5),\n",
        "        Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], max_pixel_value=255.0, p=1.0),\n",
        "        ToTensorV2(p=1.0),\n",
        "      ])\n",
        "  \n",
        "  return train_transforms\n",
        "\n",
        "\n",
        "def get_test_transforms():\n",
        "  test_transforms = Compose(\n",
        "      [\n",
        "       ToTensorV2(p=1.0)\n",
        "      ]\n",
        "  )\n",
        "  return test_transforms\n",
        "\n",
        "\n",
        "def get_light_transforms():\n",
        "  light_transforms = Compose(\n",
        "      [\n",
        "       CenterCrop(CFG.img_size, CFG.img_size),\n",
        "       ToTensorV2(p=1.0)\n",
        "      ]\n",
        "  )\n",
        "  return light_transforms"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MDj326Sruhqk"
      },
      "source": [
        "class CassavaTestDataset(Dataset):\n",
        "  \"\"\" Leaves Test Dataset \"\"\"\n",
        "  def __init__(self, img_id, transform=None):\n",
        "    self.img_id = img_id\n",
        "    self.transform = transform\n",
        "  \n",
        "  def __len__(self):\n",
        "    return len(self.img_id)\n",
        "\n",
        "  def __getitem__(self, idx): \n",
        "    img_path = CFG.test_img_path + str(self.img_id[idx])\n",
        "    img = read_img_from_path(img_path)\n",
        "\n",
        "    if self.transform:\n",
        "      img = self.transform(image=img)['image']\n",
        "      \n",
        "    return img, self.img_id[idx]\n",
        "\n",
        "\n",
        "def prepare_test_dataloader():\n",
        "  test_df = pd.read_csv(CFG.test_data_path+\"sample_submission.csv\")\n",
        "  test_id = test_df['image_id'].to_numpy()\n",
        "\n",
        "  test_dataset = CassavaTestDataset(test_id, transform=get_test_transforms())\n",
        "  test_dataloader = DataLoader(test_dataset, batch_size=CFG.batch_size, \n",
        "                               shuffle=False, num_workers=CFG.num_workers)\n",
        "  return test_dataloader"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EebzCyfwvPJ4"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CU_ekffjvKE-"
      },
      "source": [
        "class CassavaNet(nn.Module):\n",
        "  def __init__(self, model, model_name):\n",
        "    super().__init__()\n",
        "    self.model = model\n",
        "    self.model_name = model_name\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.model(x)\n",
        "  \n",
        "  def freeze(self):\n",
        "    for param in self.model.parameters():\n",
        "        param.requires_grad = False\n",
        "        \n",
        "    if 'efficientnet' in self.model_name:\n",
        "        for param in self.model.classifier.parameters():\n",
        "            param.requires_grad = True\n",
        "    elif self.model_name == 'vit_large_patch16_384' or 'deit_base_patch16_224':\n",
        "        for param in self.model.head.parameters():\n",
        "            param.requires_grad = True\n",
        "    elif 'resnext' in self.model_name:\n",
        "        for param in self.model.fc.parameters():\n",
        "            param.requires_grad = True\n",
        "  \n",
        "  def unfreeze(self):\n",
        "    for param in self.model.parameters():\n",
        "      param.requires_grad = True\n",
        "\n",
        "\n",
        "def get_resnet_model():\n",
        "  model = torchvision.models.resnet50(pretrained=CFG.pretrained)\n",
        "  model.fc = nn.Linear(2048, CFG.n_classes)\n",
        "  return model\n",
        "\n",
        "def get_resnext_model():\n",
        "  model = timm.create_model('resnext50_32x4d', pretrained=CFG.pretrained)\n",
        "  n_features = model.fc.in_features\n",
        "  model.fc = nn.Linear(n_features, CFG.n_classes)\n",
        "  return model\n",
        "\n",
        "def get_efficientnet_model():\n",
        "  model = timm.create_model('tf_efficientnet_b4_ns', pretrained=CFG.pretrained)\n",
        "  n_features = model.classifier.in_features\n",
        "  model.classifier = nn.Linear(n_features, CFG.n_classes)\n",
        "  return model\n",
        "\n",
        "def get_deit_model():\n",
        "  model = torch.hub.load('facebookresearch/deit:main', \n",
        "                                      'deit_base_patch16_384', pretrained=CFG.pretrained)\n",
        "  n_features = model.head.in_features\n",
        "  model.head = nn.Linear(n_features, CFG.n_classes)\n",
        "  return model\n",
        "\n",
        "def get_vit_model():\n",
        "  model = timm.create_model('vit_large_patch16_384', pretrained=CFG.pretrained)\n",
        "  n_features = model.head.in_features\n",
        "  model.head = nn.Linear(n_features, CFG.n_classes)\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hkPFtsGbTlkZ"
      },
      "source": [
        "def get_model(model_name):\n",
        "  model = None\n",
        "\n",
        "  if 'efficientnet' in model_name:\n",
        "    model = get_efficientnet_model()\n",
        "  elif 'deit' in model_name:\n",
        "    model = get_deit_model()\n",
        "  elif 'vit' in model_name:\n",
        "    model = get_vit_model()\n",
        "  elif 'resnext' in model_name:\n",
        "    model = get_resnext_model()\n",
        "  elif  'resnet' in model_name:\n",
        "    model = get_resnet_model()\n",
        "  else:\n",
        "    raise ValueError(\"Invalid model choice\")\n",
        "  \n",
        "  return CassavaNet(model, model_name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kgGIYMPGmEFp"
      },
      "source": [
        "def load_pretrained_models():\n",
        "  models = []\n",
        "  count = 0\n",
        "\n",
        "  for model_fpath in os.listdir(CFG.model_path):\n",
        "      full_path = CFG.model_path+model_fpath\n",
        "\n",
        "      if not os.path.isdir(full_path) and count in CFG.model_list:\n",
        "          print(\"Model Loaded:\", model_fpath)\n",
        "          model_name = model_fpath.split('_f')[0]\n",
        "          print(model_name)\n",
        "          model = get_model(model_name)\n",
        "          info = torch.load(full_path, map_location=torch.device(CFG.device))\n",
        "          model.load_state_dict(info)\n",
        "          models.append(model)\n",
        "          \n",
        "      count+=1\n",
        "  \n",
        "  return models"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O3Ig_2WqvNbp"
      },
      "source": [
        "# Inference"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p22SvVscuhn7"
      },
      "source": [
        "def infer():\n",
        "  test_dataloader = prepare_test_dataloader()\n",
        "  test_img_ids, test_pred_labels = [], []\n",
        "\n",
        "  # Construct for the purpose of testing\n",
        "  with torch.no_grad():\n",
        "    for img, img_filename in test_dataloader:\n",
        "      if CFG.use_TTA == False: # No TTA\n",
        "        voting = np.zeros((len(models), CFG.batch_size, CFG.n_classes))\n",
        "        imgs = np.zeros((CFG.batch_size, 3, CFG.img_size, CFG.img_size))\n",
        "      else: # With TTA\n",
        "        heavy_transforms = get_heavy_transforms()\n",
        "        voting = np.zeros((len(models), CFG.n_TTA, CFG.n_classes))\n",
        "        imgs = np.zeros((CFG.n_TTA, 3, CFG.img_size, CFG.img_size))\n",
        "\n",
        "        for aug_no in range(CFG.n_TTA):\n",
        "            img_np = torch.squeeze(img).numpy()\n",
        "            img_np = img_np.reshape((img_np.shape[1], img_np.shape[2], -1))\n",
        "            trans_img = heavy_transforms(image=img_np)['image']\n",
        "            imgs[aug_no, :, :, :] = trans_img.numpy()\n",
        "\n",
        "        imgs = torch.from_numpy(imgs).to(torch.float32).to(CFG.device)\n",
        "\n",
        "      # Ensemble models\n",
        "      for model_idx in range(len(models)):\n",
        "          model = models[model_idx]\n",
        "          model = model.to(CFG.device)\n",
        "          model.eval()            \n",
        "\n",
        "          logits = model(imgs)\n",
        "          voting[model_idx, :, :] = F.softmax(logits).cpu().numpy()\n",
        "\n",
        "      if CFG.use_TTA:\n",
        "        voting = np.sum(voting, axis=1)/CFG.n_TTA\n",
        "      voting = np.sum(voting, axis=0)/len(models)\n",
        "\n",
        "      pred_label = np.argmax(voting)\n",
        "      # The file name is formatted as img_id.jpeg\n",
        "      img_id = img_filename[0][:-4] \n",
        "\n",
        "      test_img_ids.append(img_id)\n",
        "      test_pred_labels.append(pred_label)\n",
        "    \n",
        "  return test_img_ids, test_pred_labels\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FTiwXsBj1bpE"
      },
      "source": [
        "# Main"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UXDZ_YEhuhs9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c265a448-f845-4624-8898-b9e91ea55b58"
      },
      "source": [
        "if __name__ == '__main__':\n",
        "  # Convert the submission dataframe to csv\n",
        "  seed_everything()\n",
        "  \n",
        "  if CFG.debug == True:\n",
        "    CFG.model_list = [0]\n",
        "\n",
        "  models = load_pretrained_models()\n",
        "  test_img_ids, test_pred_labels = infer()\n",
        "\n",
        "  output_path = '/content/submission.csv' if CFG.platform == 'colab' else '../output/kaggle/working/'\n",
        "  column_header = ['image_id', 'label']\n",
        "  submission = pd.DataFrame(zip(test_img_ids, test_pred_labels), columns=column_header)\n",
        "  submission.to_csv(path_or_buf = output_path, index = False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Loaded: resnext50_32x4d_f1_b0.894.pth\n",
            "resnext50_32x4d\n"
          ]
        }
      ]
    }
  ]
}